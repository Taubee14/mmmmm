Metadata-Version: 2.4
Name: agentbricks
Version: 0.7.1
Summary: agentbricks provides basic components that used on Bailian.platform with unified API
Home-page: https://code.alibaba-inc.com/dashscope/bailiansdk
Author: Bailian Team
Author-email: zhangzhicheng.zzc@alibaba-inc.com
License: Apache License 2.0
Keywords: python,Agent,AIGC,LLM,Components
Description-Content-Type: text/markdown
Requires-Dist: anyio
Requires-Dist: asgiref>=3.8.1
Requires-Dist: dashscope
Requires-Dist: fastapi
Requires-Dist: instructor>=1.7.9
Requires-Dist: jinja2>=3.1.6
Requires-Dist: json5
Requires-Dist: jsonref>=1.1.0
Requires-Dist: mcp
Requires-Dist: nacos-sdk-python==1.0.0
Requires-Dist: openai
Requires-Dist: opentelemetry-api
Requires-Dist: opentelemetry-exporter-otlp
Requires-Dist: opentelemetry-sdk
Requires-Dist: pydantic~=2.11.7
Requires-Dist: pytest
Requires-Dist: pytest-mock
Requires-Dist: requests
Requires-Dist: setuptools
Requires-Dist: starlette
Requires-Dist: typing_extensions
Requires-Dist: uvicorn
Requires-Dist: websockets
Requires-Dist: python-dotenv
Requires-Dist: python-multipart
Requires-Dist: pillow
Requires-Dist: azure-cognitiveservices-speech
Requires-Dist: aiohttp
Requires-Dist: urllib3>=2.1.0
Provides-Extra: sandbox
Requires-Dist: streamlit; extra == "sandbox"
Requires-Dist: e2b-desktop>=2.0.0; extra == "sandbox"
Requires-Dist: e2b-code-interpreter; extra == "sandbox"
Requires-Dist: alibabacloud_ecd20200930; extra == "sandbox"
Requires-Dist: alibabacloud_eds_aic20230930; extra == "sandbox"
Requires-Dist: alibabacloud_appstream_center20210218; extra == "sandbox"
Requires-Dist: oss2; extra == "sandbox"
Requires-Dist: wuying-agentbay-sdk==0.4.0; extra == "sandbox"
Dynamic: author
Dynamic: author-email
Dynamic: description
Dynamic: description-content-type
Dynamic: home-page
Dynamic: keywords
Dynamic: license
Dynamic: provides-extra
Dynamic: requires-dist
Dynamic: summary

# Agentbricks

<p align="center">
    <br>
    <img src="assets/banner.png"/>
    <br>
<p>
<p align="center">
<a href="https://bailian.console.aliyun.com"> Aliyun Bailian Website</a>

## What is Agentbricks?

Agentbricks is a **componentized AI application development framework** that provides enterprise-grade,
modular components for building sophisticated AI-powered applications. Built around Alibaba Cloud Bailian services,
it offers an unified API for creating scalable AI agents with production-ready capabilities.

### Key Features

- ğŸ§© **Modular Component Architecture**: Single-responsibility components with clear input/output definitions
- ğŸ”Œ **Open-Source Framework Integration**: Native support for LangGraph, AutoGen, LlamaIndex, and AgentScope, more on the road
- âš¡ **High Performance**: Async-first design with streaming support
- ğŸ”’ **Enterprise Security**: Built-in security features and error handling
- ğŸ¯ **Type Safety**: Full Pydantic integration for robust data validation

## Table of Contents

- [Getting Started](#getting-started)
  - [Installation](#installation)
  - [Quick Example](#quick-example)
- [Why Agentbricks?](#why-agentbricks)
- [Examples & Documentation](#examples--documentation)
  - [Quickstart Examples](#quickstart-examples)
  - [Complete Examples](#complete-examples)
- [Project Structure](#project-structure)
- [Contributing](#contributing)
- [License](#license)

## Getting Started

### Installation

```bash
# Basic installation
pip install agentbricks

# With sandbox support for code execution
pip install agentbricks[sandbox]

# Development installation
pip install -e .
```

**Prerequisites:**
- Python 3.10+
- `DASHSCOPE_API_KEY` environment variable for Alibaba Cloud services

### Quick Example

#### 1. Custom Component Development

Build type-safe, reusable components with automatic schema generation:

```python
import asyncio
from agentbricks.base.component import Component
from pydantic import BaseModel, Field
from typing import Any

# Define input/output schemas
class SearchInput(BaseModel):
    query: str = Field(..., description="Search query")
    max_results: int = Field(default=10, description="Maximum results")
    context: dict = Field(default_factory=dict)

class SearchOutput(BaseModel):
    results: list[str] = Field(..., description="Search results")
    total_found: int = Field(..., description="Total results found")
    context: dict = Field(..., description="Updated context")

# Implement your component
class SearchComponent(Component[SearchInput, SearchOutput]):
    async def _arun(self, args: SearchInput, **kwargs: Any) -> SearchOutput:
        # Your search logic here
        results = [f"Result for '{args.query}' #{i+1}" for i in range(args.max_results)]

        return SearchOutput(
            results=results,
            total_found=len(results),
            context={**args.context, "last_query": args.query}
        )

# Use the component
async def main():
    component = SearchComponent(
        name="Enterprise Search",
        description="High-performance search component"
    )

    result = await component.arun(SearchInput(query="AI agents", max_results=5))
    print(f"Found {result.total_found} results:")
    for r in result.results:
        print(f"  - {r}")

    # Component automatically generates OpenAI-compatible function schema
    print("\nFunction Schema:")
    print(component.function_schema)

asyncio.run(main())
```


#### 2. Open-Source Adaption

Apply component to an exists project

```python
# -*- coding: utf-8 -*-
from langchain_core.messages import HumanMessage
from langchain_openai import ChatOpenAI
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import END, START, MessagesState, StateGraph
from pydantic import BaseModel, Field
from typing import Literal, Any
import os
from agentbricks.langgraph_util import LanggraphNodeAdapter
from agentbricks.components.searches.bailian_search import BailianSearch
import asyncio

tool_node = LanggraphNodeAdapter(
    [
        BailianSearch(),
    ],
)

api_key = os.getenv("DASHSCOPE_API_KEY")

model = ChatOpenAI(
    model="qwen-turbo",
    openai_api_key=api_key,
    openai_api_base="https://dashscope.aliyuncs.com/compatible-mode/v1",
).bind_tools(tool_node.tool_schemas)


# Define the function that determines whether to continue or not
def should_continue(state: MessagesState) -> Literal["tools", END]:
    messages = state["messages"]
    last_message = messages[-1]
    # If the LLM makes a tool call, then we route to the "tools" node
    if last_message.tool_calls:
        return "tools"
    # Otherwise, we stop (reply to the user)
    return END


# Define the function that calls the model
def call_model(state: MessagesState):
    messages = state["messages"]
    response = model.invoke(messages)
    # We return a list, because this will get added to the existing list
    return {"messages": [response]}


# Define a new graph
workflow = StateGraph(MessagesState)

# Define the two nodes we will cycle between
workflow.add_node("agents", call_model)
workflow.add_node("tools", tool_node)

# Set the entrypoint as `agents`
# This means that this node is the first one called
workflow.add_edge(START, "agents")

# We now add a conditional edge
workflow.add_conditional_edges(
    # First, we define the start node. We use `agents`.
    # This means these are the edges taken after the `agents` node is called.
    "agents",
    # Next, we pass in the function that will determine which
    # node is called next.
    should_continue,
)

# We now add a normal edge from `tools` to `agents`.
# This means that after `tools` is called, `agents` node is called next.
workflow.add_edge("tools", "agents")

# Initialize memory to persist state between graph runs
checkpointer = MemorySaver()

# Finally, we compile it!
# This compiles it into a LangChain Runnable,
# meaning you can use it as you would any other runnable.
# Note that we're (optionally) passing the memory when compiling the graph
app = workflow.compile(checkpointer=checkpointer)

# Use the Runnable
final_state = app.invoke(
    {"messages": [HumanMessage(content="what is the weather in sf")]},
    config={"configurable": {"thread_id": 42}},
)
print(final_state["messages"][-1].content)


async def arun():
    final_state_async = await app.ainvoke(
        {"messages": [HumanMessage(content="what is the weather in sf")]},
        config={"configurable": {"thread_id": 42}},
    )
    print(final_state_async["messages"][-1].content)


asyncio.run(arun())

```

#### 3. Simple LLM Service

Create a production-ready LLM API service in minutes:

```python
from openai import AuthenticationError as OpenAIAuthenticationError

from agentbricks.errors import AuthenticationError, BailianError
from agentbricks.models.llm import BaseLLM
from agentbricks.schemas.bailian_llm import (
    BailianChatRequest,
    BailianParameters,
)
from agentbricks.server.fastapi_server import FastApiServer


async def general_arun(request: BailianChatRequest):
    """Test arun method"""
    llm = BaseLLM()
    parameters = BailianParameters(**request.model_dump(exclude_unset=True))
    try:
        async for chunk in llm.astream(
            model=request.model,
            messages=request.messages,
            parameters=parameters,
        ):
            yield chunk.model_dump_json(exclude_none=True)
    except OpenAIAuthenticationError as e:
        import traceback

        print(f"error {e} with traceback {traceback.format_exc()}")
        raise AuthenticationError()
    except Exception as e:
        import traceback

        print(f"error {e} with traceback {traceback.format_exc()}")
        raise BailianError(str(e))


server = FastApiServer(
    func=general_arun,
    endpoint_path="/api/v1/chat/completions",
    request_model=BailianChatRequest,
)

if __name__ == "__main__":
    server.run()
```

```bash
# Set your API key and run
export DASHSCOPE_API_KEY=your_api_key
python main.py

# Test the service
curl -X POST http://127.0.0.1:8000/api/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{"model": "qwen-max", "messages":[{"role": "user", "content":"Hello!"}]}'
```


## Why Agentbricks?

### ğŸ¯ **Production-Ready from Day One**
- Enterprise-grade components used in production by Alibaba Cloud
- Built-in observability, tracing, and error handling
- Comprehensive testing and validation

### ğŸ”§ **Framework Agnostic**
```python
# Works with your favorite frameworks
from agentbricks.agentscope_utils import agentscope_tool_adapter
from agentbricks.autogen_util import AutogenToolAdapter

# Convert to LangGraph node
node = agentscope_tool_adapter(your_component)

# Convert to AutoGen tool
tool = AutogenToolAdapter(your_component)
```

### âš¡ **High Performance**
- Async-first architecture with streaming support
- Efficient memory management and caching
- Optimized for high-throughput scenarios

### ğŸ§© **Truly Modular**
- Single-responsibility components with clear input output schemas
- Swappable implementations


## Examples & Documentation

### Quickstart Examples

| Component | Description | Link |
|-----------|-------------|------|
| **LLM Service** | OpenAI-compatible API server | [View](demos/model_api_service/) |
| **RAG System** | Document Q&A with retrieval | [View](demos/multimodal_rag/) |
| **Agent Service** | Complete agent with tools | [View](demos/agent_api_service/) |
| **Search Integration** | Enterprise search component | [View](demos/components/) |
| **Memory Systems** | Persistent conversation memory | [View](demos/components/) |

### Complete Examples

| Example | Use Case | Features | Link |
|---------|----------|----------|------|
| **Chatbot System** | Full-stack chat application | WebSocket, streaming, UI | [View](demos/chatbot_system/) |
| **Computer Use Agent** | Desktop automation agent | Screen capture, GUI control | [View](demos/computer_use/) |
| **Data Analyst** | Jupyter-powered data analysis | Code execution, visualization | [View](demos/data_analyst/) |
| **Deep Research** | Multi-step research assistant | Web search, synthesis | [View](demos/deep_research/) |
| **Realtime Voice** | Voice-to-voice AI assistant | ASR, TTS, streaming audio | [View](demos/realtime/) |
| **MCP Integration** | Model Context Protocol servers | Tool discovery, composition | [View](demos/mcp/) |
| **Framework Adapters** | LangGraph, AutoGen, LlamaIndex | Cross-framework compatibility | [View](demos/open_source_compatible/) |

### ğŸ“š **Comprehensive Documentation**

- **[Architecture Guide](docs/architecture.md)** - System design and patterns
- **[Component Development](docs/components.md)** - Building custom components
- **[Model Integration](docs/models.md)** - LLM and embedding usage
- **[Schema Reference](docs/schemas.md)** - Type definitions and validation
- **[Service Deployment](docs/services.md)** - Production deployment patterns
- **[Prompt Engineering](docs/prompts.md)** - Template-based prompt generation

## Project Structure

```
agentbricks/
â”œâ”€â”€ base/                    # Core abstractions
â”‚   â”œâ”€â”€ component.py         # Base Component class
â”‚   â”œâ”€â”€ model.py            # AI model interfaces
â”‚   â””â”€â”€ memory.py           # Memory management
â”œâ”€â”€ components/             # Production components
â”‚   â”œâ”€â”€ RAGs/               # Retrieval systems
â”‚   â”œâ”€â”€ searches/           # Search components
â”‚   â”œâ”€â”€ memory/             # Memory backends
â”‚   â”œâ”€â”€ generations/        # Content generation
â”‚   â”œâ”€â”€ sandbox_center/     # Code execution
â”‚   â”œâ”€â”€ realtime_clients/   # Audio processing
â”‚   â”œâ”€â”€ intentions/         # Intent classification
â”‚   â”œâ”€â”€ plugins/            # Plugin system
â”‚   â””â”€â”€ third_party/        # External integrations
â”œâ”€â”€ models/                 # LLM and embedding models
â”œâ”€â”€ schemas/                # Pydantic data models
â”œâ”€â”€ server/                 # FastAPI and WebSocket servers
â”œâ”€â”€ tracing/               # Observability and metrics
â”œâ”€â”€ mcp_utils/             # Model Context Protocol
â”œâ”€â”€ agents/                # Pre-built agent implementations
â””â”€â”€ utils/                 # Common utilities

demos/                     # Example applications
â”œâ”€â”€ agent_api_service/     # Production agent API
â”œâ”€â”€ chatbot_system/        # Full-stack chat app
â”œâ”€â”€ computer_use/          # Desktop automation
â”œâ”€â”€ data_analyst/          # Jupyter integration
â”œâ”€â”€ deep_research/         # Research assistant
â”œâ”€â”€ realtime/              # Voice interactions
â”œâ”€â”€ mcp/                   # MCP servers
â””â”€â”€ open_source_compatible/ # Framework adapters

docs/                      # Architecture documentation
tests/                     # Comprehensive test suite
```

## Contributing

We welcome contributions! Please see our development guidelines:

### Development Setup

```bash
# Clone the repository
git clone https://github.com/your-org/agentbricks.git
cd agentbricks

# Install in development mode
pip install -e ".[sandbox]"

# Install development dependencies
pip install -r requirements/requirements_base.txt
```

### Testing

```bash
# Run all tests
pytest
```

### Code Quality

```bash
pre-commit install
pre-commit run --all-files
```

### Guidelines

- **Component Development**: Follow the single-responsibility principle
- **Type Safety**: Use Pydantic models for all inputs/outputs
- **Testing**: Write comprehensive unit and integration tests
- **Documentation**: Update relevant docs and examples
- **Performance**: Maintain async-first patterns

### Submitting Changes

1. Fork the repository
2. Create a feature branch
3. Make your changes with tests
4. Ensure all tests pass
5. Submit a pull request

## License

Apache License 2.0 - see [LICENSE](LICENSE) for details.

---

<p align="center">
  <a href="https://bailian.console.aliyun.com">ğŸŒ Bailian Platform</a> |
  <a href="docs/architecture.md">ğŸ“– Documentation</a> |
  <a href="https://github.com/your-org/agentbricks/issues">ğŸ› Issues</a> |
  <a href="https://github.com/your-org/agentbricks/discussions">ğŸ’¬ Discussions</a>
</p>

<p align="center">
  Built with â¤ï¸ by the Alibaba Cloud Bailian Team
</p>
