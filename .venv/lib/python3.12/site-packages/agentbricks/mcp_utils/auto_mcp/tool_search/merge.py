# -*- coding: utf-8 -*-
"""
Tree merge functionality module

This module provides tree structure merging functionality, supporting:
- Hierarchical merging of multiple tool trees
- Automatic clustering and grouping
- Tree structure balance assessment and optimization
- Node index rebuilding and management
"""

import asyncio
import logging
import random
from typing import Dict, Any, List, Set, Tuple, Optional

import numpy as np

from .tree import Node, Tree
from .clustering import ClusteringManager
from .schemas import CLUSTER_TOOLS_SCHEMA
from .prompts import CLUSTER_NODES_PROMPT, CLUSTER_NODES_SYSTEM_PROMPT

logger = logging.getLogger(__name__)


class TreeStatistics:
    """Tree statistics information class"""

    def __init__(self) -> None:
        self.total_nodes = 0
        self.total_tools = 0
        self.total_directories = 0
        self.max_depth = 0
        self.tools_in_dirs = 0
        self.avg_tools_per_dir = 0.0

    @classmethod
    def calculate(cls, node: Node, depth: int = 0) -> "TreeStatistics":
        """Calculate node statistics"""
        stats = cls()
        stats.total_nodes = 1
        stats.total_tools = 0 if node.is_directory else 1
        stats.total_directories = 1 if node.is_directory else 0
        stats.max_depth = depth
        stats.tools_in_dirs = 0

        for child in node.children:
            child_stats = cls.calculate(child, depth + 1)
            stats.total_nodes += child_stats.total_nodes
            stats.total_tools += child_stats.total_tools
            stats.total_directories += child_stats.total_directories
            stats.max_depth = max(stats.max_depth, child_stats.max_depth)
            stats.tools_in_dirs += child_stats.tools_in_dirs

        if node.is_directory:
            stats.tools_in_dirs += len(
                [c for c in node.children if not c.is_directory],
            )

        stats.avg_tools_per_dir = float(stats.tools_in_dirs) / max(
            1,
            stats.total_directories,
        )
        return stats

    def log_info(self) -> None:
        """Output statistics to log"""
        logger.info("Tree statistics:")
        logger.info(f"  Total nodes: {self.total_nodes}")
        logger.info(f"  Total tools: {self.total_tools}")
        logger.info(f"  Total directories: {self.total_directories}")
        logger.info(f"  Max depth: {self.max_depth}")
        logger.info(
            f"  Average tools per directory: {self.avg_tools_per_dir:.2f}",
        )


class TreeBalanceMetrics:
    """Tree balance assessment class"""

    def __init__(
        self,
        balance_score: float,
        avg_dir_size: float,
        std_dev: float,
        max_size: int,
        min_size: int,
        avg_tools_per_dir: float,
    ):
        self.balance_score = balance_score
        self.avg_dir_size = avg_dir_size
        self.std_dev = std_dev
        self.max_size = max_size
        self.min_size = min_size
        self.avg_tools_per_dir = avg_tools_per_dir

    @classmethod
    def evaluate(cls, node: Node) -> "TreeBalanceMetrics":
        """Evaluate tree balance"""
        dir_sizes = []
        dir_tools_counts = []

        def collect_dir_stats(node: Node, level: int = 0) -> None:
            if node.is_directory:
                tool_children = [
                    child for child in node.children if not child.is_directory
                ]
                dir_children = [
                    child for child in node.children if child.is_directory
                ]

                dir_sizes.append(len(node.children))
                dir_tools_counts.append(len(tool_children))

                for child in dir_children:
                    collect_dir_stats(child, level + 1)

        collect_dir_stats(node)

        if not dir_sizes:
            return cls(1.0, 0, 0, 0, 0, 0)

        # Calculate statistical metrics
        avg_size = sum(dir_sizes) / len(dir_sizes)
        variance = sum((size - avg_size) ** 2 for size in dir_sizes) / len(
            dir_sizes,
        )
        std_dev = variance**0.5

        balance_score = (
            1.0 - (std_dev / (avg_size + std_dev)) if avg_size > 0 else 1.0
        )

        return cls(
            balance_score=balance_score,
            avg_dir_size=avg_size,
            std_dev=std_dev,
            max_size=max(dir_sizes) if dir_sizes else 0,
            min_size=min(dir_sizes) if dir_sizes else 0,
            avg_tools_per_dir=(
                sum(dir_tools_counts) / len(dir_tools_counts)
                if dir_tools_counts
                else 0
            ),
        )

    def log_info(self) -> None:
        """Output balance information to log"""
        logger.info("Tree balance assessment:")
        logger.info(f"  Balance score: {self.balance_score:.4f}")
        logger.info(f"  Average directory size: {self.avg_dir_size:.2f}")
        logger.info(f"  Standard deviation: {self.std_dev:.2f}")
        logger.info(f"  Max directory size: {self.max_size}")
        logger.info(f"  Min directory size: {self.min_size}")


class NodeIndexManager:
    """Node index manager"""

    @staticmethod
    def rebuild_index(tree: Tree, root_node: Node) -> None:
        """Rebuild name index, ensure all nodes can be correctly indexed

        This is the key method to fix node loss issues!
        """
        logger.info(
            f"Starting to rebuild name index, root node: '{root_node.name}'",
        )

        # Calculate tool count before rebuilding
        old_count = (
            len(
                [
                    name
                    for name, node in tree._name_to_node.items()
                    if not node.is_directory
                ],
            )
            if hasattr(tree, "_name_to_node")
            else 0
        )

        # Reinitialize index
        tree._name_to_node = {}

        def add_node_to_index(node: Node) -> None:
            """Recursively add nodes to index"""
            if node.name in tree._name_to_node:
                logger.warning(
                    f"{node.name} already exists in index",
                )
                # Add suffix to duplicate node names
                counter = 1
                original_name = node.name
                while node.name in tree._name_to_node:
                    node.name = f"{original_name}_{counter}"
                    counter += 1
                logger.info(f"Renamed node to: '{node.name}'")

            tree._name_to_node[node.name] = node

            # Recursively process child nodes
            for child in node.children:
                add_node_to_index(child)

        # Start rebuilding index from root node
        add_node_to_index(root_node)

        # Calculate tool count after rebuilding
        new_count = len(
            [
                name
                for name, node in tree._name_to_node.items()
                if not node.is_directory
            ],
        )

        logger.info(
            f"Name index rebuild complete, indexed {len(tree._name_to_node)}"
            "nodes",
        )
        logger.info(f"Tool node count change: {old_count} -> {new_count}")


class MergeManager(ClusteringManager):
    """Tree merge manager
    - responsible for hierarchical merging of multiple trees"""

    def __init__(self, tree_instance: Tree):
        """Initialize merge manager

        Args:
            tree_instance: Target tree instance
        """
        self.tree = tree_instance
        self.merge_config = {
            "merge_size": 3,  # Number of trees to merge per batch
            "concurrency_limit": 5,  # Concurrency limit
            "balance_threshold": 0.7,  # Balance threshold
        }

    # ==================== Main merge interface ====================

    async def merge_trees(
        self,
        tree_list: List[Node],
        schema: dict = CLUSTER_TOOLS_SCHEMA,
        system_prompt: str = CLUSTER_NODES_SYSTEM_PROMPT,
        user_prompt: str = CLUSTER_NODES_PROMPT,
    ) -> Tree:
        """Main entry method for merging multiple trees

        Uses divide-and-conquer strategy to merge multiple tool trees
          suitable for large searches tree merging.

        Args:
            tree_list: List of tree nodes to merge
            schema: JSON Schema for validating LLM response
            system_prompt: System prompt template
            user_prompt: User prompt template

        Returns:
            Merged tree instance
        """
        logger.info(f"Starting to merge {len(tree_list)} subtrees...")

        # Count input tools
        input_tool_count = sum(
            self._count_tools_in_tree(tree) for tree in tree_list
        )
        logger.info(
            f"Input trees contain {input_tool_count} tool nodes in total",
        )

        # 1. Initialize new root node
        new_root = Node(
            name="root",
            description="Tool root node",
            is_directory=True,
        )

        # 2. Execute hierarchical merge, directly return final root node
        final_root = await self._execute_hierarchical_merge(
            tree_list,
            new_root,
            schema,
            system_prompt,
            user_prompt,
        )

        # Check final tool count after merge
        final_tool_count = self._count_tools_in_tree(final_root)
        logger.info(
            f"merge complete, final contains {final_tool_count} tool nodes",
        )

        if final_tool_count != input_tool_count:
            logger.warning(
                f"Lost {input_tool_count - final_tool_count} tools "
                "during merge!",
            )

        # 3. Post-processing and optimization
        await self._post_process_tree(
            final_root,
            schema,
            system_prompt,
            user_prompt,
        )

        # Final verification
        post_process_tool_count = self._count_tools_in_tree(final_root)

        if post_process_tool_count != input_tool_count:
            logger.error(
                f"Total lost {input_tool_count - post_process_tool_count}"
                " tools during merge process!",
            )

        return self.tree

    async def _execute_hierarchical_merge(
        self,
        tree_list: List[Node],
        final_root: Node,
        schema: dict,
        system_prompt: str,
        user_prompt: str,
    ) -> Node:
        """Execute hierarchical merge process
        Merge layer by layer until final merge to specified root node
        """
        current_trees = tree_list.copy()
        semaphore = asyncio.Semaphore(
            int(self.merge_config["concurrency_limit"]),
        )
        depth = 0

        def count_total_children(trees: List[Node]) -> int:
            """Count total child nodes of all trees"""
            total_children = 0
            for tree in trees:
                total_children += len(tree.children)
            return total_children

        total_children = count_total_children(current_trees)

        while total_children > self.tree.max_items_per_node * 2:
            depth += 1
            # Create batches and execute merge
            batches = self._create_balanced_batches(current_trees)

            batch_results = await self._process_merge_batches(
                batches,
                depth,
                semaphore,
                schema,
                system_prompt,
                user_prompt,
            )

            # Handle unclassified tool nodes
            current_trees = await self._handle_unclassified_tools(
                batch_results,
                depth,
            )
            total_children = count_total_children(current_trees)

        # Final merge to specified root node
        final_children = count_total_children(current_trees)
        logger.info(
            (
                f"Starting final merge, merging {len(current_trees)} trees "
                f"(total {final_children} child nodes) to root node"
            ),
        )

        # If only one tree, directly move its child nodes to root node
        single_tree = current_trees[0]
        # Create copy of child nodes to avoid modifying set during iteration
        children_copy = list(single_tree.children)
        for child in children_copy:
            self.tree.add_child(child, final_root)
        logger.info(
            f"Single tree direct move, total {len(children_copy)} child nodes",
        )

        return final_root

    def _create_balanced_batches(
        self,
        tree_list: List[Node],
    ) -> List[List[Node]]:
        """Create balanced batches for merging

        Args:
            tree_list: List of tree nodes to batch (tree nodes as roots)

        Returns:
            Two-dimensional list after batching
        """
        merge_size = self.merge_config["merge_size"]
        batches = []

        # Simple batching strategy: group by merge_size
        for i in range(0, len(tree_list), int(merge_size)):
            batch = tree_list[i : i + int(merge_size)]
            if len(batch) >= 2:  # Only need to merge if 2 or more nodes
                batches.append(batch)
            else:
                # Keep single nodes directly
                if batches:
                    batches[-1].extend(batch)
                else:
                    batches.append(batch)

        return batches

    async def _process_merge_batches(
        self,
        batches: List[List[Node]],
        depth: int,
        semaphore: asyncio.Semaphore,
        schema: dict,
        system_prompt: str,
        user_prompt: str,
    ) -> List[Node]:
        """Process merge batches

        Args:
            batches: List of batches
            depth: Current merge depth
            semaphore: Concurrency control semaphore
            schema: JSON Schema
            system_prompt: System prompt
            user_prompt: User prompt

        Returns:
            List of merged nodes
        """

        async def process_single_batch(
            batch: List[Node],
        ) -> Node:  # Single node processing
            async with semaphore:
                # if len(batch) == 1:
                #     return batch[0]

                # Create temporary root node for merging
                temp_root = Node(
                    name=f"temp_merge_d{depth}",
                    description=f"Layer {depth} temporary merge node",
                    is_directory=True,
                )

                await self._merge_subtrees(
                    batch,
                    temp_root,
                    schema,
                    system_prompt,
                    user_prompt,
                )

                return temp_root

        # Process all batches concurrently
        tasks = [process_single_batch(batch) for batch in batches]
        return await asyncio.gather(*tasks)

    async def _handle_unclassified_tools(
        self,
        batch_results: List[Node],
        depth: int,
    ) -> List[Node]:
        """Handle unclassified tool nodes

        Args:
            batch_results: Batch merge results
            depth: Current depth

        Returns:
            List of processed nodes
        """
        return [node for node in batch_results if node is not None]

    async def _post_process_tree(
        self,
        root_node: Node,
        schema: dict,
        system_prompt: str,
        user_prompt: str,
    ) -> None:
        """Post-process tree structure

        Args:
            root_node: Root node
            schema: JSON Schema
            system_prompt: System prompt
            user_prompt: User prompt
        """
        # 1. Rebuild name index
        NodeIndexManager.rebuild_index(self.tree, root_node)
        self.tree.root = root_node

        # 2. Clean empty nodes
        self.tree.clean_node(root_node)

        # 3. Evaluate tree balance
        balance_metrics = TreeBalanceMetrics.evaluate(root_node)
        balance_metrics.log_info()

        # 4. If balance is insufficient, perform rebalancing
        if (
            balance_metrics.balance_score
            < self.merge_config["balance_threshold"]
        ):
            logger.info("Tree structure unbalanced, starting rebalancing...")
            await self.rebalance_tree(
                threshold=self.merge_config["balance_threshold"],
                schema=schema,
                system_prompt=system_prompt,
                user_prompt=user_prompt,
            )

        # 5. Output final statistics
        final_stats = TreeStatistics.calculate(root_node)
        final_stats.log_info()

    # ==================== Subtree merge core logic ====================

    async def _merge_subtrees(
        self,
        nodes_list: List[Node],
        new_root: Node,
        schema: dict,
        system_prompt: str,
        user_prompt: str,
    ) -> Node:
        """Recursively merge multiple subtree nodes

        Merge multiple subtrees under a new root node,
          preserving all node information

        Args:
            nodes_list: List of nodes to merge
            new_root: New root node
            schema: JSON Schema for validating LLM response
            system_prompt: System prompt template
            user_prompt: User prompt template
        """

        # 1. Collect all child nodes and node mapping relationships
        all_children, node_children_map = self._collect_children_from_nodes(
            nodes_list,
        )

        origin_tools = 0
        for cluster in all_children:
            cluster_tools = self._count_tools_in_tree(cluster)
            origin_tools += cluster_tools

        if not all_children:
            logger.info("No child nodes found, merge complete")
            return new_root

        # 2. If child node count is reasonable, add directly
        if len(all_children) <= self.tree.max_items_per_node:

            for child in all_children:
                self.tree.add_child(child, new_root)

            return new_root

        # 3. Cluster child nodes
        logger.info(
            f"Child node count({len(all_children)}) exceeds limit,"
            " starting clustering",
        )
        clusters = await self.cluster_nodes_llm(
            all_children,
            schema=schema,
            system_prompt=system_prompt,
            user_prompt=user_prompt,
        )

        # Verify tool node count after clustering
        total_clustered_tools = 0
        for cluster in clusters:
            cluster_tools = self._count_tools_in_tree(cluster)
            total_clustered_tools += cluster_tools

        # Check if clustering was successful
        if total_clustered_tools != origin_tools or not clusters:
            logger.warning(
                "Using improved fallback strategy: "
                "group and add by max_items_per_node limit",
            )

            # Improved fallback strategy: group by max_items_per_node limit
            await self._add_clusters_with_grouping(
                list(all_children),
                new_root,
                schema,
                system_prompt,
                user_prompt,
            )

            # Verify fallback strategy results
            return new_root

        await self._process_cluster_results(
            list(clusters),
            nodes_list,
            node_children_map,
            schema,
            system_prompt,
            user_prompt,
        )

        # 5. Add clusters to new root node, create grouping layer if necessary
        await self._add_clusters_with_grouping(
            list(clusters),
            new_root,
            schema,
            system_prompt,
            user_prompt,
        )

        # Final verification
        return new_root

    def _count_tools_in_tree(
        self,
        node: Node,
        visited: Optional[Set[Node]] = None,
    ) -> int:
        """Recursively count tool nodes in tree

        Args:
            node: Node to count
            visited: Set of visited nodes, used to prevent circular references

        Returns:
            Number of tool nodes
        """
        if visited is None:
            visited = set()

        if node in visited:
            return 0

        visited.add(node)

        if not node.is_directory:
            return 1

        count = 0
        for child in node.children:
            count += self._count_tools_in_tree(child, visited)

        visited.remove(
            node,
        )  # Remove during backtracking, allow revisit in other paths
        return count

    def _collect_children_from_nodes(
        self,
        nodes_list: List[Node],
    ) -> Tuple[Set[Node], Dict[Node, Set[Node]]]:
        """Collect all child nodes and mapping relationships from node list

        Args:
            nodes_list: List of nodes

        Returns:
            (Set of all child nodes, mapping dict from node to child nodes)
        """
        all_children = set()
        node_children_map = {}

        logger.info(
            f"Starting to collect child nodes from {len(nodes_list)} nodes",
        )

        for i, node in enumerate(nodes_list):
            logger.info(
                (
                    f"Processing node {i + 1}/{len(nodes_list)}: "
                    f"'{node.name}' "
                    f"(directory: {node.is_directory}, "
                    f"child: {len(node.children)})"
                ),
            )

            if node.is_directory:
                # Record directory node's child node mapping
                node_children = set(node.children)
                all_children.update(node_children)
                node_children_map[node] = node_children

                if node.parent:
                    node.parent.children.discard(node)
                # Only delete if it actually exists in index
                if (
                    hasattr(self.tree, "_name_to_node")
                    and node.name in self.tree._name_to_node
                ):
                    del self.tree._name_to_node[node.name]
                    logger.debug(f"  Removed '{node.name}' from index")
            else:
                # Tool nodes added directly to child node set
                all_children.add(node)
                logger.info(f"  Tool node '{node.name}' added directly")

        # Count collection results
        tool_children = [
            child for child in all_children if not child.is_directory
        ]
        dir_children = [child for child in all_children if child.is_directory]

        logger.info(
            f"Collection complete: total {len(all_children)} child nodes",
        )
        logger.info(f"  Tool nodes: {len(tool_children)}")
        logger.info(f"  Directory nodes: {len(dir_children)}")
        logger.info(f"  From {len(node_children_map)} directory nodes")

        return all_children, node_children_map

    async def _process_cluster_results(
        self,
        clusters: List[Node],
        original_nodes: List[Node],
        node_children_map: Dict[Node, Set[Node]],
        schema: dict,
        system_prompt: str,
        user_prompt: str,
    ) -> None:
        """Process clustering results,
        identify and handle parts that need recursive merging

        Args:
            clusters: Clustering results
            original_nodes: Original node list
            node_children_map: Node to child nodes mapping
            schema: JSON Schema
            system_prompt: System prompt
            user_prompt: User prompt
        """
        for cluster in clusters:
            # Count original nodes that intersect with current cluster
            intersecting_nodes = []
            intersecting_children = []

            for node in original_nodes:
                if node in node_children_map:
                    # Get intersection of directory child nodes
                    original_dirs = {
                        x for x in node_children_map[node] if x.is_directory
                    }
                    cluster_dirs = {
                        x for x in cluster.children if x.is_directory
                    }
                    intersection = cluster_dirs.intersection(original_dirs)

                    if intersection:
                        intersecting_nodes.append(node)
                        intersecting_children.extend(intersection)

            # If multiple nodes have intersections, need recursive merging
            if len(intersecting_nodes) > 1:
                await self._merge_subtrees(
                    intersecting_children,
                    cluster,
                    schema=schema,
                    system_prompt=system_prompt,
                    user_prompt=user_prompt,
                )

    async def _add_clusters_with_grouping(
        self,
        clusters: List[Node],
        parent_node: Node,
        schema: dict,
        system_prompt: str,
        user_prompt: str,
    ) -> None:
        """Add clusters to parent node,
        create intermediate grouping layer if necessary

        Args:
            clusters: List of cluster nodes
            parent_node: Parent node
            schema: JSON Schema for validating LLM response
            system_prompt: System prompt template
            user_prompt: User prompt template
        """
        if not clusters:
            logger.warning("No clusters to add to parent node")
            return

        if len(clusters) <= self.tree.max_items_per_node * 2:
            logger.info(
                f"Cluster count({len(clusters)}) within limit,"
                " adding directly to parent node",
            )
            # Slightly relaxed, not strictly enforced
            for cluster in clusters:
                self.tree.add_child(cluster, parent_node)
            return

        logger.info(
            (
                f"{parent_node.name}: Cluster count({len(clusters)}) exceeds "
                f"max limit({self.tree.max_items_per_node}),"
                "creating intermediate grouping layer"
            ),
        )

        # If cluster count exceeds limit, force grouping processing
        await self._force_group_clusters(
            clusters,
            parent_node,
            schema,
            system_prompt,
            user_prompt,
        )

    async def _force_group_clusters(
        self,
        clusters: List[Node],
        parent_node: Node,
        schema: dict,
        system_prompt: str,
        user_prompt: str,
    ) -> None:
        """Force grouping processing for clusters exceeding limit

        Args:
            clusters: List of cluster nodes
            parent_node: Parent node
            schema: JSON Schema
            system_prompt: System prompt
            user_prompt: User prompt
        """
        # Try using LLM for secondary clustering

        meta_clusters = await self._create_meta_clusters(
            clusters,
            schema,
            system_prompt,
            user_prompt,
        )
        logger.info(
            f"{parent_node.name} -created- {len(meta_clusters)} meta clusters",
        )

        for meta_cluster in meta_clusters:
            self.tree.add_child(meta_cluster, parent_node)
        return

    async def _create_meta_clusters(
        self,
        clusters: List[Node],
        schema: dict,
        system_prompt: str,
        user_prompt: str,
    ) -> Set[Node]:
        """Perform secondary clustering on clusters, create meta clusters

        Args:
            clusters: Original cluster list
            schema: JSON Schema for validating LLM response
            system_prompt: System prompt template
            user_prompt: User prompt template

        Returns:
            Meta cluster list
        """
        logger.info(
            "Starting to create meta clusters"
            f", input cluster count: {len(clusters)}",
        )

        # 1. Generate enhanced descriptions for clusters

        # 2. Use enhanced descriptions for secondary clustering
        meta_cluster_nodes = await self.cluster_nodes_llm(
            set(clusters),
            schema=schema,
            system_prompt=system_prompt,
            user_prompt=user_prompt,
        )

        return meta_cluster_nodes

    # TODO Currently only basic framework

    def _print_merge_stats(self) -> None:
        """Print merge result statistics"""
        if hasattr(self.tree, "root") and self.tree.root:
            stats = TreeStatistics.calculate(self.tree.root)
            stats.log_info()
        else:
            logger.warning(
                "Cannot print statistics: tree root node does not exist",
            )

    def evaluate_tree_balance(
        self,
        node: Node | None = None,
    ) -> TreeBalanceMetrics:
        """Evaluate tree balance

        Calculate child node distribution for each directory node
        and overall tree balance metrics

        Args:
            node: Root node to evaluate, defaults to tree's root node

        Returns:
            TreeBalanceMetrics object
        """
        if node is None:
            node = self.tree.root

        return TreeBalanceMetrics.evaluate(node)

    # ==================== Tree rebalancing methods ====================

    async def rebalance_tree(
        self,
        threshold: float = 0.6,
        schema: dict = {},
        system_prompt: str | None = None,
        user_prompt: str | None = None,
    ) -> Tree:
        """Rebalance tree structure

        If tree balance is below specified threshold,
        attempt to rebalance tree structure

        Args:
            threshold: Balance threshold,
            rebalancing triggered when below this value
            schema: JSON Schema for validating LLM response
            system_prompt: System prompt template
            user_prompt: User prompt template

        Returns:
            Rebalanced tree structure
        """
        # Evaluate current balance
        balance_metrics = TreeBalanceMetrics.evaluate(self.tree.root)
        balance_metrics.log_info()

        if balance_metrics.balance_score >= threshold:
            logger.info(
                "Tree structure already sufficiently balanced, "
                "no rebalancing needed",
            )
            return self.tree

        # Identify and process unbalanced nodes
        unbalanced_nodes = self._find_unbalanced_nodes(balance_metrics)
        # Filter out nodes that don't allow splitting
        allowed_unbalanced_nodes = [
            (node, size)
            for node, size in unbalanced_nodes
            if getattr(node, "allow_split", True)
        ]

        await self._process_unbalanced_nodes(
            allowed_unbalanced_nodes,
            balance_metrics,
        )

        # Re-evaluate balance
        new_metrics = TreeBalanceMetrics.evaluate(self.tree.root)
        logger.info(
            f"Tree balance after rebalancing: {new_metrics.balance_score:.4f}",
        )

        # Clean possible empty nodes
        self.tree.clean_node(self.tree.root)

        return self.tree

    def _find_unbalanced_nodes(
        self,
        balance_metrics: TreeBalanceMetrics,
    ) -> List[Tuple[Node, int]]:
        """Identify unbalanced nodes

        Args:
            balance_metrics: Balance metrics

        Returns:
            List of (node, node size) tuples
        """
        unbalanced_dirs = []
        target_size = balance_metrics.avg_dir_size

        def collect_unbalanced_dirs(node: Node) -> None:
            if node.is_directory:
                dir_size = len(node.children)
                if abs(dir_size - target_size) > balance_metrics.std_dev * 1.5:
                    unbalanced_dirs.append((node, dir_size))

                # Recursively check subdirectories
                for child in node.children:
                    if child.is_directory:
                        collect_unbalanced_dirs(child)

        collect_unbalanced_dirs(self.tree.root)

        # Sort by degree of difference, process most unbalanced nodes first
        unbalanced_dirs.sort(
            key=lambda x: abs(x[1] - target_size),
            reverse=True,
        )

        return unbalanced_dirs

    async def _process_unbalanced_nodes(
        self,
        unbalanced_nodes: List[Tuple[Node, int]],
        balance_metrics: TreeBalanceMetrics,
    ) -> None:
        """Process unbalanced nodes

        Args:
            unbalanced_nodes: List of unbalanced nodes
            balance_metrics: Balance metrics
            schema: JSON Schema
            system_prompt: System prompt
            user_prompt: User prompt
        """
        if not unbalanced_nodes:
            logger.info("No nodes found that need rebalancing")
            return

        # Limit number of nodes processed each time to avoid over-processing
        max_nodes_to_process = min(5, len(unbalanced_nodes))

        for node, size in unbalanced_nodes[:max_nodes_to_process]:
            logger.info(
                f"Rebalancing node '{node.name}', current size: {size}",
            )

            if size > balance_metrics.avg_dir_size * 1.5:  # Node too large
                # Check if node allows splitting
                if getattr(node, "allow_split", True):
                    await self._handle_oversized_node(
                        node,
                    )
                else:
                    logger.info(
                        f"Node '{node.name}' is oversized but "
                        "splitting not allowed, skipping",
                    )
            elif (
                size < balance_metrics.avg_dir_size * 0.5 and node.parent
            ):  # Node too small
                await self._handle_undersized_node(node)

    async def _handle_oversized_node(
        self,
        node: Node,
    ) -> None:
        """Handle oversized nodes"""
        logger.info(f"Splitting oversized node '{node.name}'")
        await self._split_directory(node)

    async def _handle_undersized_node(self, node: Node) -> None:
        """Handle undersized nodes"""
        if not node.parent:
            logger.info(f"Node '{node.name}' has no parent, skipping merge")
            return

        siblings = [
            child
            for child in node.parent.children
            if child.is_directory and child != node
        ]
        if siblings:
            target_sibling = min(
                siblings,
                key=lambda s: len(s.children),
            )  # Choose smallest sibling node

            # Move node's child nodes to target sibling node
            for child in list(node.children):
                self.tree.add_child(child, target_sibling)

            # Remove original node from parent node
            node.parent.children.discard(node)
            if node.name in self.tree._name_to_node:
                del self.tree._name_to_node[node.name]

    # ==================== Directory splitting methods ====================

    async def _split_directory(self, ori_directory: Node) -> None:
        """Split directory node, handle unbalanced directory structure

        When directory node is unbalanced, continuously merge its child nodes

        Args:
            ori_directory: Original directory node
        """
        if not ori_directory.is_directory:
            logger.warning(
                f"Node '{ori_directory.name}' is not a directory,"
                " skipping split",
            )
            return

        directory = ori_directory
        count = 0

        while (
            directory and count < 3
        ):  # Limit recursion depth to prevent infinite loops
            if len(directory.children) > self.tree.max_items_per_node:
                # Check if node allows splitting
                if getattr(directory, "allow_split", True):
                    logger.info(
                        (
                            f"Splitting directory '{directory.name}', "
                            f"child node count: {len(directory.children)}"
                        ),
                    )
                    await self._split_directory_single(directory)

                    # Clean structure after splitting
                    if directory.parent:
                        self.tree.clean_node(directory.parent)

                    # Check if need to continue processing upward
                    if (
                        directory.parent
                        and len(directory.parent.children)
                        <= self.tree.max_items_per_node
                    ):
                        directory = directory.parent
                        count = 0
                    else:
                        count += 1
                else:
                    logger.info(
                        f"Directory '{directory.name}' does not allow"
                        " splitting, skipping split operation",
                    )
                    break  # Stop processing if splitting not allowed
            else:
                directory = directory.parent
                count = 0

    async def _split_directory_single(self, directory: Node) -> None:
        """Split single directory node containing too many items

        Args:
            directory: Directory node to split
        """
        if (
            not directory.is_directory
            or len(directory.children) <= self.tree.max_items_per_node
        ):
            return

        # Check if node allows splitting
        if not getattr(directory, "allow_split", True):
            logger.info(
                f"Directory '{directory.name}' does not allow splitting,"
                "skipping split operation",
            )
            return

        children = directory.children.copy()

        # Optional: print debug info
        if hasattr(directory, "visualize_tree"):
            directory.visualize_tree(debug=True, description=False)

        # Use clustering algorithm to regroup child nodes
        try:
            clusters = await self.cluster_nodes_llm(
                children,
                schema=CLUSTER_TOOLS_SCHEMA,
                system_prompt=CLUSTER_NODES_SYSTEM_PROMPT,
                user_prompt=CLUSTER_NODES_PROMPT,
            )

            # Add clustering results to appropriate parent node
            if directory.parent:
                # Has parent node, add to parent node
                for cluster in clusters:
                    self.tree.add_child(cluster, directory.parent)
            else:
                # No parent node (root node), add directly to current directory
                for cluster in clusters:
                    self.tree.add_child(cluster, directory)
                logger.info(f"Added {len(clusters)} clusters to root node")

            # Record processed tools
            tool_names = [x.name for x in children if not x.is_directory]
            if tool_names:
                logger.info(f"Processed the following tools: {tool_names}")

        except Exception as e:
            logger.error(
                f"Error while splitting directory '{directory.name}': {e}",
            )
            # When error occurs, don't split, maintain original structure
