# -*- coding: utf-8 -*-
"""
Embedding vector generations and processing utilities
"""

import numpy as np
import asyncio
from typing import List, Optional, Any


class EmbeddingProcessor:
    """Embedding vector processing utility class"""

    def __init__(self, client: Any) -> None:
        self.client = client
        self.embed_model = "text-embedding-v3"

    async def get_embeddings_batch(
        self,
        texts: List[str],
        batch_size: int = 10,
    ) -> np.ndarray:
        """Batch get embedding vectors for texts,
        with concurrent processing support

        Args:
            texts: List of texts to get embedding vectors for
            batch_size: Number of texts to process per batch

        Returns:
            numpy array containing all embedding vectors
        """
        all_embeddings = []
        semaphore = asyncio.Semaphore(3)  # Limit concurrent requests

        async def process_batch(batch_texts: List[str]) -> List[Any]:
            async with semaphore:
                try:
                    response = await asyncio.to_thread(
                        self.client.embeddings.create,
                        model=self.embed_model,
                        input=batch_texts,
                    )
                    return [data.embedding for data in response.data]
                except Exception as e:
                    print(f"Error getting embeddings: {e}")
                    # Return empty vectors as fallback
                    return [None] * len(batch_texts)

        # Process in batches
        batches = []
        for i in range(0, len(texts), batch_size):
            batch = texts[i : i + batch_size]
            batches.append(process_batch(batch))

        print(f"Processing embeddings in {len(batches)} concurrent batches...")
        results = await asyncio.gather(*batches)

        # Merge results
        for batch_result in results:
            all_embeddings.extend(batch_result)

        # Handle possible None values
        dim = 1536  # Default embedding dimension
        if any(e is not None for e in all_embeddings):
            for e in all_embeddings:
                if e is not None:
                    dim = len(e)
                    break
        for i, emb in enumerate(all_embeddings):
            if emb is None:
                all_embeddings[i] = np.zeros(dim)

        return np.array(all_embeddings)

    @staticmethod
    def cosine_similarity(vec1: np.ndarray, vec2: np.ndarray) -> float:
        """Calculate cosine similarity between two vectors"""
        if vec1 is None or vec2 is None:
            return 0.0
        dot_product = np.dot(vec1, vec2)
        norm1 = np.linalg.norm(vec1)
        norm2 = np.linalg.norm(vec2)
        if norm1 == 0 or norm2 == 0:
            return 0.0
        return dot_product / (norm1 * norm2)

    @staticmethod
    def batch_similarity(
        query_vec: list,
        vectors: list,
    ) -> List[float] | np.ndarray:
        """Batch calculate vector similarities"""
        # Handle zero vector cases
        if query_vec is None or vectors is None:
            return []
        # Calculate dot product
        scores = np.array(query_vec) @ np.array(vectors).T
        return scores
