# -*- coding: utf-8 -*-
import asyncio
import json
import sys
import os
import logging
from tqdm import tqdm
from dotenv import load_dotenv
from .tree import Tree, Node
from typing import Tuple, Optional, Any
from jsonschema import validate
import numpy as np
from collections import defaultdict
from .search import SearchManager
from .merge import MergeManager
from .mcp_adapter import MCPConnector

# Import Schema definitions and prompts
from .schemas import (
    NODE_CLASSIFICATION_SCHEMA,
)
from .prompts import (
    NODE_CLASSIFICATION_PROMPT,
    NODE_CLASSIFICATION_SYSTEM_PROMPT,
)

load_dotenv()

# Configure logging
logger = logging.getLogger(__name__)
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler(os.path.join(os.getcwd(), "tree_search.log")),
    ],
)


class MCP_Tree(Tree):
    def __init__(
        self,
        batch_size: int = 10,
        max_items_per_node: int = 5,
        base_url: Optional[str] = None,
        openai_api_key: Optional[str] = None,
        model: Optional[str] = None,
        embed_model: Optional[str] = None,
        embedding_cache_path: Optional[str] = None,
        server_script_path: Optional[str] = None,
        **kwargs: Any,
    ) -> None:
        """Initialize MCP_Tree

        Args:
            batch_size: Batch processing size
            max_items_per_node: Maximum number of items per node
            base_url: OpenAI API base URL
            openai_api_key: OpenAI API key
            model: Model name to use
            embed_model: Model name for generating embeddings
            embedding_cache_path: Embedding cache file path
            server_script_path: MCP server script path, auto-connect
            if provided
            **kwargs: Other parameters passed to parent class
        """
        super().__init__(
            max_items_per_node=max_items_per_node,
            batch_size=batch_size,
            embedding_cache_path=embedding_cache_path,
            base_url=base_url,
            openai_api_key=openai_api_key,
            model=model,
            embed_model=embed_model,
            **kwargs,
        )

        self.search = SearchManager(self)
        # Initialize managers
        self.mergemanager = MergeManager(self)
        self.mcp_connector = MCPConnector()

        # Auto-connect if server script path is provided
        self.server_script_path = server_script_path
        self._server_connected = False

    async def connect_to_server(
        self,
        server_script_path: str | None = None,
    ) -> bool:
        """Connect to MCP server and list available tools"""
        if server_script_path is None:
            server_script_path = self.server_script_path

        if not server_script_path:
            logger.warning("未提供 MCP 服务器脚本路径，跳过连接")
            return False

        try:
            # 检查脚本类型
            await self.mcp_connector.connect(server_script_path)
            # 列出MCP服务器上的工具
            tools = await self.mcp_connector.get_tools()
            logger.info(
                f"\n已连接到服务器，支持以下工具: {[tool['name'] for tool in tools]}",
            )
            if tools:
                logger.info(f"工具示例: {tools[0]}")
            self._server_connected = True
            return True
        except Exception as e:
            logger.error(f"连接到 MCP 服务器失败: {e}")
            self._server_connected = False
            return False

    def clean_node(self, node: Node | None) -> None:
        """Clean up nodes"""
        # Remove empty child nodes
        if not node or not node.is_directory:
            return
        children = node.children
        if node.is_directory and not children:
            if node.parent:
                node.parent.children.discard(node)
            node.parent = None
            # Delete node
            if node.name in self._name_to_node:
                del self._name_to_node[node.name]

        elif len(children) > 1:
            for child in list(children):
                if child.is_directory:
                    self.clean_node(child)

        else:
            while len(children) == 1 and list(children)[0].is_directory:
                # If only one child node, promote child node directly
                if list(children)[0].name in self._name_to_node:
                    del self._name_to_node[list(children)[0].name]
                children = list(children)[0].children

            children_to_add = list(children)
            for child in children_to_add:
                self.add_child(child, node)
                self.clean_node(child)

    async def classify_nodes_batch(
        self,
        all_nodes_data: list,
        root: Node,
    ) -> dict:
        """Batch classify nodes using LLM

        Args:
            all_nodes_data: All node data that needs to be classified

        Returns:
            Node classification result list
        """
        # 当前树结构
        tree_now = root.visualize_tree(debug=True)
        # 定义JSON schema
        json_schema = NODE_CLASSIFICATION_SCHEMA

        results = defaultdict(list)

        # 创建并发任务
        async def process_batch(data: dict) -> dict:
            # 限制并发请求数
            async with asyncio.Semaphore(5):
                try:
                    prompt = NODE_CLASSIFICATION_PROMPT.format(
                        tree_structure=tree_now,
                        node_data=data,
                    )
                    content = NODE_CLASSIFICATION_SYSTEM_PROMPT.format(
                        json_schema=json_schema,
                    )
                    messages = [
                        {
                            "role": "system",
                            "content": content,
                        },
                        {"role": "user", "content": prompt},
                    ]
                    # 带重试的LLM请求
                    retries = 3
                    while retries > 0:
                        try:
                            response = await self.ask_llm_async(
                                messages=messages,
                                model="qwen-plus-latest",
                                response_format={
                                    "type": "json_schema",
                                    "json_schema": {
                                        "name": "response",
                                        "schema": json_schema,
                                    },
                                    "strict": True,
                                },
                            )
                            parse_result = json.loads(response)
                            validate(instance=parse_result, schema=json_schema)
                            return {**parse_result, "child": data["name"]}
                        except Exception as e:
                            logger.warning(
                                f"LLM归类失败，剩余重试次数: {retries - 1}, "
                                f"错误: {e}",
                            )
                            await asyncio.sleep(1)
                            retries -= 1

                    # 如果所有重试都失败，回退到默认归类（全部放在根节点下）
                    return {"parent": "root", "child": data["name"]}

                except Exception as e:
                    logger.error(f"处理批次时发生错误: {e}")
                    # 回退策略：全部归类到根节点
                    return {"parent": "root", "child": data["name"]}

        # 并行处理所有批次
        batch_results = await asyncio.gather(
            *(process_batch(batch) for batch in all_nodes_data),
        )

        # 合并结果
        for result in batch_results:
            results[result["parent"]].append(result["child"])

        return results

    async def add_nodes(
        self,
        nodes_data: list,
        root: Node,
        method: str = "embedding",
    ) -> bool:
        """Add nodes to appropriate positions in the tree"""
        # Generate node embeddings
        if method == "LLM":
            # LLM方法并行处理
            if not root:
                # 如果树是空的，直接放在根节点下
                parse = {root.name: [n["name"] for n in nodes_data]}
            else:
                # 并行使用LLM进行归类
                parse = await self.classify_nodes_batch(nodes_data, root)

        # 插入节点到树中，树结构会自动处理分裂逻辑
        await self.insert_nodes(parse, nodes_data)
        self.clean_node(root)
        return True

    async def insert_nodes(self, parse: dict, nodes_data: list) -> None:
        """Insert nodes into the tree"""
        # Parse node data
        nodes_dic = {node["name"]: Node(**node) for node in nodes_data}
        parse_list = []
        for parent_name in parse:  # 每个父节点处理一次
            children = parse[parent_name]
            parent_node = self.get_node_by_name(parent_name)
            parse_list.append(parent_node)
            if parent_node:
                # 将子节点添加到父节点
                for child in children:

                    child_node = nodes_dic.get(child)
                    if child_node:
                        self.add_child(child_node, parent_node)
        # print("---+++初试插入+++---")
        # self.root.visualize_tree(debug=True, description=False)
        for parent_node in parse_list:
            # 处理过多子节点的目录
            if parent_node:
                await self.mergemanager._split_directory(parent_node)

    async def build_tree_merge(
        self,
        save_path: str | None = None,
        additional_tools: list | None = None,
    ) -> dict:
        """Build merge version of tree structure,
        supporting additional tool data input

        Args:
            save_path: Save path
            additional_tools: Additional tool data list,
            no additional tools added if None
        """
        logger.info("正在获取可用工具...")
        try:
            all_tools = await self.mcp_connector.get_tools()
        except Exception as e:
            logger.error(f"获取工具失败: {e}")
            all_tools = []
        # 如果提供了额外的工具数据，合并到all_tools中
        if additional_tools:
            for tool in additional_tools:
                if tool["name"] not in [x["name"] for x in all_tools]:
                    all_tools.append(tool)

        all_tools = all_tools
        # 去重
        logger.info(f"找到 {len(all_tools)} 个工具，开始构建工具树...")
        self.tree_node = [tool["name"] for tool in all_tools]
        logger.info(f"去重后工具数量: {len(set(self.tree_node))}")
        # 使用并发批处理获取嵌入向量
        logger.info("正在获取工具名称的嵌入向量...")
        tmp_emb = await self.get_embeddings_batch(
            self.tree_node,
            batch_size=10,
        )

        self.tree_emb = np.zeros((3 * len(tmp_emb), tmp_emb.shape[1]))
        self.tree_emb[: len(tmp_emb)] = tmp_emb
        logger.info(f"获取嵌入向量完成，形状: {tmp_emb.shape}")

        # 步骤1：基于前缀和服务器名称对工具进行预聚类
        logger.info("开始基于前缀和服务器名称对工具进行预聚类...")
        prefix_groups, server_groups, ungrouped_tools = (
            self._preprocess_tools_by_similarity(all_tools)
        )

        # 合并所有分组结果
        all_grouped_tools = []
        total_groups = 0

        # 处理前缀分组
        for prefix, tools in prefix_groups.items():
            if len(tools) > 1:
                total_groups += 1
                group_node = Node(
                    name=f"prefix_group_{prefix}_{total_groups}",
                    description=f"具有'{prefix}'前缀的工具组",
                    is_directory=True,
                )
                for tool in tools:
                    tool_node = Node(**tool)
                    self.add_child(tool_node, group_node)
                all_grouped_tools.append(group_node)
                logger.info(f"创建前缀组 '{prefix}': {len(tools)} 个工具")
            else:
                # 单个工具直接加入未分组列表
                ungrouped_tools.extend(tools)

        # 处理服务器分组
        for server, tools in server_groups.items():
            total_groups += 1
            server_name = tools[0]["name"].split("#")[0] if tools else server
            server_des = (
                tools[0]["description"].split("#")[0].replace("\n", "\t")
                if tools
                else f"来自'{server}'服务的工具组"
            )
            group_node = Node(
                name=server_name,
                description=server_des,
                is_directory=True,
                allow_split=False,  # 服务器节点不允许分割，保证完整性
            )
            for tool in tools:
                # 只保留工具描述的后半部分，并确保移除换行符
                tool["description"] = (
                    tool["description"].split("#")[-1].replace("\n", "\t")
                )
                tool_node = Node(**tool)
                self.add_child(tool_node, group_node)
            all_grouped_tools.append(group_node)

        # 步骤2：对分组的server按批次大小分组
        batch_nodes = []  # 先添加预聚类的组
        for i in range(0, len(all_grouped_tools), self.batch_size):
            batch = all_grouped_tools[i : i + self.batch_size]

            temp_node = Node(
                name=f"server_batch_{i}",
                description="server归并节点",
                is_directory=True,
            )
            for node in batch:
                self.add_child(node, temp_node)
            batch_nodes.append(temp_node)
            logger.info(
                f"创建服务组 '{temp_node.name}': {len(batch)} 个server"
                f"（作为独立单元处理）",
            )

        # 对未分组的工具按批次大小分组
        for i in range(0, len(ungrouped_tools), self.batch_size):
            batch = ungrouped_tools[i : i + self.batch_size]
            logger.info(
                (
                    f"创建未分组工具批次 {i // self.batch_size + 1}/"
                    f"{(len(ungrouped_tools) - 1) // self.batch_size + 1} "
                    f"({len(batch)}个工具)"
                ),
            )
            temp_node = Node(
                name=f"temp_batch_{i}",
                description="未分组工具批次节点",
                is_directory=True,
            )
            for tool in batch:
                tool_node = Node(**tool)
                self.add_child(tool_node, temp_node)
            batch_nodes.append(temp_node)

        logger.info(
            f"预聚类完成: {len(prefix_groups)}个前缀组, "
            f"{len(server_groups)}个服务组, {len(ungrouped_tools)}个未分组工具",
        )

        # 等待所有批次完成，并收集结果
        logger.info("所有批次处理完成，开始合并子树...")

        await self.mergemanager.merge_trees(batch_nodes)

        # 显示树的统计信息
        stats = self.get_directory_stats()
        logger.info("\n树结构统计:")
        logger.info(f"总工具数: {stats['total_tools']}")
        logger.info(f"总目录数: {stats['total_directories']}")
        logger.info(f"最大深度: {stats['max_depth']}")
        logger.info(
            f"每个目录平均工具数: " f"{stats['avg_tools_per_dir']:.2f}",
        )

        logger.info("树构建完成!")

        # 保存树结构到文件
        if save_path is None:
            save_path = os.path.join(
                os.path.dirname(__file__),
                "tool_tree.json",
            )

        self.save_tree(save_path)
        logger.info(f"树结构已保存到: {save_path}")

        return self.to_dict()

    def save_tree(self, filepath: str) -> bool:
        """Save tree structure to JSON file

        Args:
            filepath: File path to save
        """
        tree_data = {
            "tree": self.to_dict(),
            "tree_emb": (
                self.tree_emb.tolist()
                if isinstance(self.tree_emb, np.ndarray)
                else self.tree_emb
            ),
            "tree_node": self.tree_node,
            "batch_size": self.batch_size,
            "max_items_per_node": self.max_items_per_node,
        }

        try:
            with open(filepath, "w", encoding="utf-8") as f:
                json.dump(tree_data, f, ensure_ascii=False, indent=2)
            return True
        except Exception as e:
            print(f"保存树结构时发生错误: {e}")
            return False

    @classmethod
    def load_tree(cls, filepath: str) -> Optional["MCP_Tree"]:
        """Load tree structure from file

        Args:
            filepath: Tree structure file path

        Returns:
            Loaded MCP_Tree instance
        """
        try:
            with open(filepath, "r", encoding="utf-8") as f:
                tree_data = json.load(f)

            # 创建新的树实例
            batch_size = tree_data.get("batch_size", 10)
            max_items_per_node = tree_data.get("max_items_per_node", 5)
            tree = cls(
                batch_size=batch_size,
                max_items_per_node=max_items_per_node,
            )

            # 恢复embedding和节点名称列表
            tree.tree_emb = (
                np.array(tree_data["tree_emb"])
                if tree_data.get("tree_emb")
                else []
            )
            tree.tree_node = tree_data.get("tree_node", [])

            # 递归构建树结构
            tree._rebuild_tree_from_dict(tree_data["tree"])

            print(f"成功从 {filepath} 加载树结构")
            return tree

        except Exception as e:
            print(f"加载树结构时发生错误: {e}")
            return None

    def _rebuild_tree_from_dict(
        self,
        node_dict: dict,
        parent: Node | None = None,
    ) -> Node:
        """Rebuild tree structure from dictionary

        Args:
            node_dict: Dictionary representing the node
            parent: Parent node (None if root node)

        Returns:
            Rebuilt node
        """
        # 如果是恢复根节点
        if parent is None:
            self.root = self._dict_to_node(node_dict)
            self._name_to_node = {"root": self.root}

            # 递归构建子节点
            for child_dict in node_dict.get("children", []):
                self._rebuild_tree_from_dict(child_dict, self.root)

            return self.root

        # 构建非根节点
        node = self._dict_to_node(node_dict)
        node.parent = parent

        # 添加到名称索引
        self._name_to_node[node.name] = node

        # 添加到父节点的子节点列表
        parent.children.add(node)

        # 递归构建子节点
        for child_dict in node_dict.get("children", []):
            self._rebuild_tree_from_dict(child_dict, node)

        return node

    def _dict_to_node(self, node_dict: dict) -> Node:
        """Create node from dictionary

        Args:
            node_dict: Dictionary representing the node

        Returns:
            Created Node instance
        """
        # 转换embedding（如果存在）
        embedding = node_dict.get("embedding")
        if embedding is not None:
            embedding = np.array(embedding)

        # 创建节点（忽略children，因为会在递归中添加）
        node = Node(
            name=node_dict.get("name", ""),
            description=node_dict.get("description", ""),
            embedding=embedding,
            is_directory=node_dict.get("is_directory", False),
            children=None,  # 子节点在递归过程中添加
            parent=None,  # 父节点在调用者中设置
        )

        return node

    async def cleanup(self) -> None:
        """Clean up resources"""
        await self.mcp_connector.cleanup()

    def _preprocess_tools_by_similarity(
        self,
        tools_list: list,
    ) -> Tuple[dict, dict, list]:
        """Preprocess tools based on tool name prefixes and server names
        to optimize clustering

        Args:
            tools_list: Tool list

        Returns:
            tuple: (prefix_groups, server_groups, ungrouped_tools)
        """
        prefix_groups = {}
        server_groups = {}
        ungrouped_tools = []

        for tool in tools_list:
            grouped = False

            # 首先尝试按服务器名称分组（优先级高）
            if "#" in tool["name"]:
                # 如果工具名包含#，提取服务器名称
                server_name = tool["name"].split("#")[0].lower()
                if server_name not in server_groups:
                    server_groups[server_name] = []
                server_groups[server_name].append(tool)
                grouped = True

            # 未分组的工具
            if not grouped:
                ungrouped_tools.append(tool)

        # 过滤只有一个工具的前缀组（这些工具加入未分组列表）
        # 但保留所有服务器组，无论工具数量多少
        filtered_prefix_groups = {
            k: v for k, v in prefix_groups.items() if len(v) > 1
        }
        filtered_server_groups = {
            k: v for k, v in server_groups.items() if len(v) > 1
        }

        # 将单个工具的组加入未分组列表
        for group in prefix_groups.values():
            if len(group) == 1:
                ungrouped_tools.extend(group)
        for group in server_groups.values():
            if len(group) == 1:
                ungrouped_tools.extend(group)
        logger.info(
            f"预处理统计: 前缀组{len(filtered_prefix_groups)}个"
            f", 服务组{len(filtered_server_groups)}个, 未分组{len(ungrouped_tools)}个",
        )

        return filtered_prefix_groups, filtered_server_groups, ungrouped_tools


async def main() -> None:
    """Main function"""
    if len(sys.argv) > 1:
        if sys.argv[1] == "--load":
            # Load tree from file
            tree_file = sys.argv[2] if len(sys.argv) > 2 else "tool_tree.json"
            client = MCP_Tree.load_tree(tree_file)
            if client:

                if len(sys.argv) > 3 and sys.argv[3] == "--searches":
                    # Execute searches
                    client.root.print_tree_stats_summary()

                    query = (
                        " ".join(sys.argv[4:])
                        if len(sys.argv) > 4
                        else input("请输入搜索查询: ")
                    )
                    await client.search.hybrid_multi_tool_search(
                        query,
                        max_results=5,
                    )

        elif sys.argv[1] == "--searches" and len(sys.argv) > 2:
            # Load tree and searches directly
            client = MCP_Tree.load_tree("tool_tree.json")
            if client:
                query = " ".join(sys.argv[2:])
                await client.search.hybrid_multi_tool_search(
                    query,
                    max_results=5,
                )
    else:
        # Build new tree
        client = MCP_Tree()
        try:
            await client.connect_to_server("searches/mcp_server.py")
            # await client.build_tree(method="LLM", save_path="tool_tree.json")
            await client.build_tree_merge(
                save_path="tool_tree_merge.json",
            )
            client.root.visualize_tree(
                debug=True,
                save_path="visualize_tree.txt",
            )
        finally:
            await client.cleanup()


if __name__ == "__main__":
    asyncio.run(main())
